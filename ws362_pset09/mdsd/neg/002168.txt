that information is the opposite of entropy which is a measure of disorder or uncertainty. However because this book is about complexity and not information per se, I will only briefly refer to his mistakes with the latter as I have explained them further in other reviews that are specifically on that topic. 
 
Shannon's information rate from communications theory, R, is an entropy like formula but most critically it is a state function difference of the uncertainty reduction to a recognizer after a measurement. Entropy is not a proper measure of disorder or uncertainty; the 2nd law of entropy increase of the universe applied long before there were any observers. It is a measure of the dispersal of energy. Going back in time is not going back to perfect order, but quite the opposite. I have not seen proper definitions in any book but there are PhD level articles available on the internet with proper definitions such as the Principia Cybernetica Web and molecular biologist Dr Thomas Schneider's website. Biologist Richard Dawkins also has an accurate short article on the internet. Most physicists have the definitions wrong unfortunately and believe information evolved before life, which is false. (A recognizer is required, whether a ribosome or mind etc.) Instead a better definition of complexity than the present author offers would indicate that the universe has increased in complexity through gravitational clumping (among other things). By making the mistake then the physicists and present author believe maximum information is randomness or equilibrium. This is the definition of algorithmic complexity.
 
As the author adapts algorithmic theory to his complexity profile he arrives at formulas that are observer dependant: "the complexity profile [is] the length of the description [of] the error allowed [as] the description increases."   This is of little or no practical use. Again the universe has grown in complexity (or at least in pockets or we wouldn't be here) without relying on the degree of focus of any observer. A crystal is highly ordered relative to say a human cell whose complexity is a result of a multitude of interactions of chemical agents and macro molecules. This is where his analysis falls silent, in fact wrong. He says (page 741) "short-range correlations decrease the microstate complexity..." Well that's because he has a flawed method of using statistical mechanics. There is likely no universal complexity algorithm. Consider that a single gene can yield up to thousands of different proteins. One should be wary however of any formula that treats correlations as reduced complexity! Again the crystal vs the cell!
 
However there are ways of measuring the critical biological requirement of interactions that in fact increase complexity, the opposite of equilibrium statistical mechanics, a flawed tool. For instance in a recent article at lanl.arXiv.org, authors Edwin Wang et. al. apply Pearson's correlation coefficient to show that "genes with higher cis-regulation complexity are more coordinately regulated by transcription factors at the transcriptional level and by micro RNA's at the post-transcriptional level. This is a potentially novel discovery of a mechanism for coordinated regulation of gene expression...We found a positive correlation between these two  groups of transcriptional regulators... " Measures of correlations are key in studying biological complexity and are not based on an observer's focus ability.
 
For a layman's guide to the issue of correlations for life see Irun Cohen's book 'Tending Adam's Garden' (though it has no quantitative aspect).
